{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Unificado2 - Nat y MJ.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Importamos librerías"
      ],
      "metadata": {
        "id": "NS80kz2SsClE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import os\n",
        "import tweepy as tw\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import openpyxl"
      ],
      "outputs": [],
      "metadata": {
        "id": "36fgHz-Qr7nW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72714274-429e-48c6-9edf-3ae2fca78522"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Credenciales para el uso de la API\n"
      ],
      "metadata": {
        "id": "ugBpSobvsMEP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "consumer_key= 'qLBuvGr2AzL5TmARGF2vIYLar'\n",
        "consumer_secret= 'WwPMJxsf7ofyYLsqEr1Tj8dy636eT9D5Jmgp4ab2JgjT5RpQJo'\n",
        "access_token= '1267192889416847362-4WE0cPAp56rWJ7sGedH909p8QC4P0v'\n",
        "access_token_secret= 'ByZnnbxuiRoBNbfoWJTCj6xiXuwCwoLorwVOOfZPD44ln'"
      ],
      "outputs": [],
      "metadata": {
        "id": "pQNk2jW8sPAw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "N_YulbAEsSaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funcion que genere fechas-horas aleatorias \n",
        "- El rango será entre el 1 de enero de 2020 y el 15 de julio de 2021\n",
        "- Se generará una fecha y esta será el parámetro *toDate*\n",
        "- Se aleatorizará de la siguiente manera:\n",
        "    · año -> 2020 o 2021\\\n",
        "    · mes:\\\n",
        "        >>> si año = 2020 -> aleatorio de 1 a 12\\\n",
        "        >>> si año = 2021 -> aleatorio de 1 a 7\\\n",
        "    · dia:\\\n",
        "           >>>  si mes = 1 / 3 / 5 / 7 / 8 / 10 / 12 -> aleatorio de 1 a 31\\\n",
        "           >>>  si mes = 4 / 6 / 9 / 11 -> aleatorio de 1 a 30\\\n",
        "           >>>  si mes = 2 -> aleatorio de 1 a 28\\\n",
        "    · hora -> de 0 a 24\\\n",
        "    · minutos -> constante a 00"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "def generate_toDate():\n",
        "\n",
        "    # Generar año\n",
        "    year =  str(np.random.choice([2020, 2021]))\n",
        "\n",
        "    # Generar mes\n",
        "    if year == '2020':\n",
        "        month = str(np.random.randint(low=1, high=12))\n",
        "    else:\n",
        "        month = str(np.random.randint(low=1, high=7))\n",
        "    if len(month) == 1: month = '0' + month\n",
        "    \n",
        "    # Generar dia\n",
        "    if month in ['01', '03', '05', '07', '08', '10', '12']:\n",
        "        day = str(np.random.randint(low=1, high=31))\n",
        "    elif month in ['04', '06', '09', '11']:\n",
        "        day = str(np.random.randint(low=1, high=30))\n",
        "    else:\n",
        "        day = str(np.random.randint(low=1, high=28))\n",
        "    if len(day) == 1: day = '0' + day\n",
        "\n",
        "    # Generar hora\n",
        "    hour = str(np.random.randint(low=0, high=24))\n",
        "    if len(hour) == 1: hour = '0' + hour\n",
        "\n",
        "    \n",
        "    toDate = year + month + day + hour + '00'\n",
        "\n",
        "    return toDate"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXTRACCION FULL ARCHIVE"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "query_word = 'telefonica OR movistar'\n",
        "until = '202103281200'"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "source": [
        "x = api.search_full_archive(environment_name='fullarchive', query=query_word,\n",
        "                            toDate=until, maxResults=100)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "def tweets_to_DF(tweets):\n",
        "    \n",
        "    df=pd.DataFrame(columns=['tweet_id', 'text', 'created_at', 'retweeted', 'retweet_count', 'favorite_count','user_ verified',\n",
        "                             'user_id', 'user_name', 'user_location', 'geo', 'coordinates', 'place', 'user_notificacion', \n",
        "                             'user_followers', 'user_friends','user_withheld_in_countries', 'is_reply', 'finished_tweet',\n",
        "                             'status_count'])    \n",
        "    i=0\n",
        "    for tweet in tweets:\n",
        "\n",
        "        new_row = {\n",
        "        'tweet_id': tweet._json['id'],\n",
        "        'text': tweet._json['text'],\n",
        "        'created_at': tweet._json['created_at'],\n",
        "        'retweeted': tweet._json['retweeted'],\n",
        "        'retweet_count': tweet._json['retweet_count'],\n",
        "        'favorite_count': tweet._json['favorite_count'],\n",
        "        'user_verified': tweet._json['user']['verified'], \n",
        "        'user_id': tweet._json['user']['id'],\n",
        "        'user_name': tweet._json['user']['screen_name'],\n",
        "        'user_location': tweet._json['user']['location'],\n",
        "        'geo': tweet._json['geo'],\n",
        "        'coordinates': tweet._json['coordinates'],\n",
        "        'place': tweet._json['place'],\n",
        "        'user_notificacion': tweet._json['user']['notifications'],\n",
        "        'user_followers': tweet._json['user']['followers_count'],\n",
        "        'user_friends': tweet._json['user']['friends_count'],\n",
        "        'user_withheld_in_countries':  tweet._json['user']['withheld_in_countries'],\n",
        "        'is_reply': tweet._json['in_reply_to_status_id'],\n",
        "        'finished_tweet': tweet._json['truncated'],\n",
        "        'status_count':tweet._json['user']['statuses_count']\n",
        "        }\n",
        "        \n",
        "        df.loc[i] = new_row # Añadimos la info al dataframe en una nueva fila\n",
        "        i = i + 1\n",
        "\n",
        "    return df\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTANTE!\n",
        "Cada vez que se haga una extracción, crear un nuevo dataframe y guardarlo en excel de la siguiente manera: \\\n",
        "\\\n",
        "    *df_fa_x = tweets_to_DF(x)* \\\n",
        "    *df_fa_x.to_excel('extracciones/df_fa_x.xlsx')* \\\n",
        "    \\\n",
        "**Sustituir 'x' por un número siguiendo una numeración consecutiva**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "source": [
        "df_fa_1 = tweets_to_DF(x)\n",
        "df_fa_1.to_excel('extracciones/df_fa_1.xlsx')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combinar DataFrames con las extracciones de prueba. Con esto se conforma el dataframe al que se le irán añadiendo las futuras extracciones:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "df_fa_1 = pd.read_excel('extracciones/df_fa_1.xlsx')\n",
        "df_fa_2 = pd.read_excel('extracciones/df_fa_2.xlsx')\n",
        "df_fa_3 = pd.read_excel('extracciones/df_fa_3.xlsx')\n",
        "df_fa_4 = pd.read_excel('extracciones/df_fa_4.xlsx')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "#NO VOLVER A EJECUTAR\n",
        "df_fa = df_fa_1.append(df_fa_2, ignore_index=True)\n",
        "df_fa = df_fa.append(df_fa_3, ignore_index=True)\n",
        "df_fa = df_fa.append(df_fa_4, ignore_index=True)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bucle para automatizar la extraccion con las siguientes operaciones:\n",
        "- Obtener *toDate* aleatoria\n",
        "- Ejecutar *api.search_full_archive*\n",
        "- Generar dataframe a partir de los tweets extraidos\n",
        "- Guardar extraccion en excel\n",
        "- Añadir extraccion a DataFrame conjunto *df_fa*\n",
        "\n",
        "## IMPORTANTE: Por seguridad, al acabar un bucle, guardamos *df_fa* en excel"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTANTISIMO: Antes de ejecutar el bucle, asegurarse que df_fa está cargado con todas las extracciones y revisar que la variable n que se utiliza en **'file_name'** es el correcto para que las extracciones se vayan guardando en excel con numeracion consecutiva"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "for i in range(6):\n",
        "    \n",
        "    # Obtener toDate aleatoria\n",
        "    to_date = generate_toDate()\n",
        "\n",
        "    # Ejecutar api.search_full_archive\n",
        "    x = api.search_full_archive(environment_name='fullarchive', query='telefonica OR movistar',\n",
        "                            toDate=to_date, maxResults=100)\n",
        "                            \n",
        "    # Generar dataframe a partir de los tweets extraidos\n",
        "    df_fa_n = tweets_to_DF(x)\n",
        "\n",
        "    # Guardar extraccion en excel #ACTUALIZAR NUM 4 POR NUMERO DE ARCHIVOS YA EXTRAIDOS!!!!!\n",
        "    file_name = 'extracciones/df_fa_' + str(i+1+4) + '.xlsx'\n",
        "    df_fa_n.to_excel(file_name)\n",
        "\n",
        "    # Añadir extraccion al DataFrame conjunto 'df_fa'\n",
        "    df_fa = df_fa.append(df_fa_n, ignore_index=True)\n",
        "\n",
        "    \n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardado de el df conjunto por seguridad"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "df_fa.to_excel('extracciones/df_fa_v1000.xlsx')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    }
  ]
}